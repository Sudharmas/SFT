# -*- coding: utf-8 -*-
"""SFT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f7aRhM0VBYfbTkYDT-vRYZ_isdT-vRap
"""

# Commented out IPython magic to ensure Python compatibility.
# 
# 
# %%capture
# !pip install pymupdf pandas python-docx openpyxl
# 
# import fitz
# import pandas as pd
# from docx import Document
# import os
# import glob
# 
# 
# def read_pdf(file_path):
#     """Extracts text from PDF, removing extra whitespace."""
#     doc = fitz.open(file_path)
#     text = ""
#     for page in doc:
#         text += page.get_text()
#     return text
# 
# def read_docx(file_path):
#     """Extracts text from Word Documents."""
#     doc = Document(file_path)
#     return "\n".join([para.text for para in doc.paragraphs if para.text])
# 
# def read_excel(file_path):
#     """Converts Excel rows into string representation."""
#     df = pd.read_excel(file_path)
#     text_data = []
#     for _, row in df.iterrows():
#         row_str = ", ".join([f"{col}: {val}" for col, val in row.items() if pd.notna(val)])
#         text_data.append(row_str)
#     return "\n".join(text_data)
# 
# def process_directory(directory_path):
#     """Loops through a folder and extracts text from all supported files."""
#     raw_data = []
# 
#     extensions = ['*.pdf', '*.docx', '*.xlsx']
#     files = []
#     for ext in extensions:
#         files.extend(glob.glob(os.path.join(directory_path, ext)))
# 
#     print(f"Found {len(files)} files to process.")
# 
#     for file in files:
#         print(f"Processing: {file}")
#         try:
#             if file.endswith('.pdf'):
#                 content = read_pdf(file)
#             elif file.endswith('.docx'):
#                 content = read_docx(file)
#             elif file.endswith('.xlsx'):
#                 content = read_excel(file)
#             raw_data.append({"filename": file, "content": content})
#         except Exception as e:
#             print(f"Error processing {file}: {e}")
# 
#     return raw_data
# 
# print("Ingestion pipeline ready. Upload your files to the Colab 'Files' tab.")

import torch
try:
    major_version, minor_version = torch.cuda.get_device_capability()
    print(f"GPU Detected: {torch.cuda.get_device_name(0)}")
except:
    print("No GPU detected. Ensure Runtime > Change Runtime Type is set to GPU.")

!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

!pip install --no-deps xformers trl peft accelerate bitsandbytes

print("Installation Complete.")

# Commented out IPython magic to ensure Python compatibility.
# 
# %%capture
# !pip install llama-parse llama-index-core nest_asyncio pandas python-docx openpyxl
# 
# import nest_asyncio
# nest_asyncio.apply()
# 
# import os
# import glob
# import pandas as pd
# from docx import Document
# from llama_parse import LlamaParse
# 
# api_key = "llx-2tNMntcRo6lJYDamKrbtQAFIU2U3kTv28VC9g30k7nGsk7t2"
# 
# os.environ["LLAMA_CLOUD_API_KEY"] = api_key
# 
# 
# def read_pdf_llama(file_path):
#     """
#     Uses LlamaParse to extract text while preserving table structure
#     as Markdown (pipes | and dashes -).
#     """
#     print(f"   ...Sending {os.path.basename(file_path)} to LlamaCloud...")
# 
#     parser = LlamaParse(
#         result_type="markdown",
#         verbose=False,
#         language="en"
#     )
# 
#     try:
#         documents = parser.load_data(file_path)
#         full_text = "\n\n".join([doc.text for doc in documents])
#         return full_text
#     except Exception as e:
#         print(f"   ‚ùå LlamaParse Error: {e}")
#         return ""
# 
# def read_docx(file_path):
#     """Extracts text from Word Documents."""
#     doc = Document(file_path)
#     return "\n".join([para.text for para in doc.paragraphs if para.text])
# 
# def read_excel(file_path):
#     """Converts Excel rows into string representation."""
#     df = pd.read_excel(file_path)
#     text_data = []
#     for _, row in df.iterrows():
#         row_str = ", ".join([f"{col}: {val}" for col, val in row.items() if pd.notna(val)])
#         text_data.append(row_str)
#     return "\n".join(text_data)
# 
# def process_directory(directory_path="."):
#     """Loops through folder and extracts text using the best tool for each file."""
#     raw_data = []
#     extensions = ['*.pdf', '*.docx', '*.xlsx']
#     files = []
# 
#     for ext in extensions:
#         files.extend(glob.glob(os.path.join(directory_path, ext)))
# 
#     print(f"Found {len(files)} files to process.")
# 
#     for file in files:
#         print(f"Processing: {file}")
#         content = ""
#         try:
#             if file.endswith('.pdf'):
#                 content = read_pdf_llama(file)
#             elif file.endswith('.docx'):
#                 content = read_docx(file)
#             elif file.endswith('.xlsx'):
#                 content = read_excel(file)
# 
#             if content:
#                 raw_data.append({"filename": file, "content": content})
#                 print(f"   ‚úÖ Success: {len(content)} chars extracted.")
#             else:
#                 print("   ‚ö†Ô∏è Warning: No content extracted.")
# 
#         except Exception as e:
#             print(f"Error processing {file}: {e}")
# 
#     return raw_data
# 
# if api_key == "PASTE_YOUR_LLAMA_INDEX_API_KEY_HERE":
#     print("‚ùå STOP: You must paste your API Key in the 'api_key' variable at the top of the script!")
# else:
#     print("Starting LlamaParse Ingestion Engine...")
#     ingested_data = process_directory("./data")
# 
#     if ingested_data:
#         print("\n--- SAMPLE OUTPUT (First 500 chars) ---")
#         print(ingested_data[0]['content'][:500])
#         if "|" in ingested_data[0]['content']:
#             print("\n‚úÖ Tables detected in Markdown format.")

import json

CHUNK_SIZE = 1024
OVERLAP = 100

def create_chunks(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):
    """
    Splits text into overlapping chunks.
    """
    chunks = []
    start = 0
    text_len = len(text)

    while start < text_len:
        end = start + chunk_size
        chunk = text[start:end]

        if end < text_len:
            last_space = chunk.rfind(' ')
            if last_space != -1:
                end = start + last_space
                chunk = text[start:end]

        chunks.append(chunk)
        start = end - overlap

    return chunks

print("‚öôÔ∏è Processing Data into Training Examples...")
training_data = []

if 'ingested_data' not in globals():
    print("‚ùå Error: 'ingested_data' not found. Please run the Ingestion script first.")
else:
    for doc in ingested_data:
        filename = doc['filename']
        content = doc['content']

        doc_chunks = create_chunks(content)
        print(f"   üìÑ {filename}: Created {len(doc_chunks)} chunks.")

        for i, chunk in enumerate(doc_chunks):
            entry = {
                "instruction": f"You are an expert technical assistant. Provide details from the document '{filename}'.",
                "input": "",
                "output": chunk
            }
            training_data.append(entry)

    output_file = "dataset.json"
    with open(output_file, "w") as f:
        json.dump(training_data, f, indent=2)

    print(f"\n‚úÖ SUCCESS: Dataset saved as '{output_file}' with {len(training_data)} training examples.")
    print("   Ready for Model Selection.")

from unsloth import FastLanguageModel
import torch
from tqdm import tqdm


max_seq_length = 2048
dtype = None
load_in_4bit = True

print("üöÄ Loading Base Model for Data Generation...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-Instruct-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
FastLanguageModel.for_inference(model)

generation_prompt = """You are an expert data analyst.
Below is a technical excerpt from a product manual.
Generate 5 distinct, specific questions that a customer might ask, which can be answered ONLY using this excerpt.
Do not provide the answers, only the questions.

### Excerpt:
{}

### Questions:
1."""

enhanced_data = []
print(f"\n‚öôÔ∏è Generating Synthetic Data from {len(training_data)} chunks...")

for entry in tqdm(training_data):
    context = entry['output']
    inputs = tokenizer(
        [generation_prompt.format(context)],
        return_tensors = "pt"
    ).to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens = 256,
        use_cache = True
    )
    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

    raw_questions = generated_text.split("### Questions:")[-1].strip()

    questions = [q.strip() for q in raw_questions.split('\n') if q.strip() and (q[0].isdigit() or q.startswith('-'))]

    for q in questions:
        clean_q = q.lstrip("1234567890.- ")

        new_entry = {
            "instruction": clean_q,
            "input": "",
            "output": context
        }
        enhanced_data.append(new_entry)

import json
combined_data = training_data + enhanced_data

with open("enhanced_dataset.json", "w") as f:
    json.dump(combined_data, f, indent=2)

print(f"\n‚úÖ GENERATION COMPLETE!")
print(f"Original Chunks: {len(training_data)}")
print(f"Synthetic Questions Created: {len(enhanced_data)}")
print(f"Total Training Data: {len(combined_data)}")

from unsloth import FastLanguageModel
import torch

max_seq_length = 2048
dtype = None
load_in_4bit = True

print("üß† Loading Llama-3-8B-Instruct Architecture...")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-Instruct-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

print("‚úÖ Model Architecture Ready. Adapters attached.")

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
from datasets import load_dataset

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

dataset = load_dataset("json", data_files="enhanced_dataset.json", split="train")
dataset = dataset.map(formatting_prompts_func, batched = True,)

print(f"‚úÖ Dataset Loaded: {len(dataset)} examples ready for training.")

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
    ),
)

print("üöÄ Training Started... (This takes 2-5 mins)")
trainer_stats = trainer.train()
print("üèÜ Training Complete!")



import re

FastLanguageModel.for_inference(model)

topic = "frequency response"
product = "DesignMax DM8SE"

question = f"What is the {topic} of the {product}?"
style_instruction = "Extract the specific value and answer in a single, plain English sentence. Do NOT use markdown tables, pipes (|), or lists."

prompt = alpaca_prompt.format(
    f"{question}\n\n{style_instruction}",
    "",
    "",
)

inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    use_cache=True,
    temperature=0.1,
)

def clean_output(text):
    response = text.split("### Response:")[-1].strip()

    response = response.replace("|", "")
    response = response.replace("---", "")

    response = re.sub(' +', ' ', response)

    response = re.sub(r'\'', '', response)

    return response.strip()

final_answer = clean_output(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])

print("\n" + "="*50)
print(f"ü§ñ AI ASSISTANT REPORT")
print("="*50)
print(f"üìù Query: {question}")
print("-" * 50)
print(f"üí¨ Answer:\n")
print(final_answer)
print("\n" + "="*50)





import json
import os
import glob
from llama_parse import LlamaParse
import nest_asyncio
nest_asyncio.apply()

current_dataset_file = "enhanced_dataset.json"
new_files_directory = "new_docs"
output_file = "combined_dataset_v2.json"

os.makedirs(new_files_directory, exist_ok=True)

print(f"Instructions: Upload ONLY your NEW files into the '{new_files_directory}' folder on the left.")
input("üëâ Press ENTER once you have uploaded the NEW files...")

def parse_new_files():
    print(f"üöÄ Parsing new files from {new_files_directory}...")
    parser = LlamaParse(result_type="markdown", verbose=True, language="en")

    new_docs = []
    files = glob.glob(os.path.join(new_files_directory, "*.pdf"))

    for file in files:
        try:
            documents = parser.load_data(file)
            full_text = "\n\n".join([doc.text for doc in documents])
            new_docs.append({"filename": file, "content": full_text})
            print(f"   ‚úÖ Parsed: {file}")
        except Exception as e:
            print(f"   ‚ùå Failed: {file} - {e}")

    return new_docs

def process_to_json(raw_docs):
    formatted_entries = []
    CHUNK_SIZE = 1024
    OVERLAP = 100

    for doc in raw_docs:
        text = doc['content']
        filename = doc['filename']
        start = 0
        while start < len(text):
            end = start + CHUNK_SIZE
            chunk = text[start:end]
            formatted_entries.append({
                "instruction": f"You are an expert technical assistant. Provide details from '{filename}'.",
                "input": "",
                "output": chunk
            })
            start = end - OVERLAP
    return formatted_entries

if not os.path.exists(current_dataset_file):
    print(f"‚ö†Ô∏è Warning: Old dataset '{current_dataset_file}' not found. Starting fresh.")
    old_data = []
else:
    with open(current_dataset_file, "r") as f:
        old_data = json.load(f)
    print(f"üìö Loaded {len(old_data)} existing examples.")

raw_new_docs = parse_new_files()
if raw_new_docs:
    new_json_entries = process_to_json(raw_new_docs)

    combined_data = old_data + new_json_entries

    with open(output_file, "w") as f:
        json.dump(combined_data, f, indent=2)

    print(f"\n‚úÖ SUCCESS!")
    print(f"   Old Data: {len(old_data)}")
    print(f"   New Data: {len(new_json_entries)}")
    print(f"   Total Data: {len(combined_data)}")
    print(f"   Saved as: '{output_file}' (Use this for training now)")
else:
    print("No new files processed.")

from unsloth import FastLanguageModel, is_bfloat16_supported
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

max_seq_length = 2048
dtype = None
load_in_4bit = True

print("üß† Loading Base Model...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-Instruct-bnb-4bit",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

print("üõ†Ô∏è Attaching LoRA Adapters...")
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

try:
    dataset = load_dataset("json", data_files="combined_dataset_v2.json", split="train")
    print(f"üìö Loaded Combined Dataset with {len(dataset)} examples.")
except:
    print("‚ö†Ô∏è 'combined_dataset_v2.json' not found. Loading 'enhanced_dataset.json' instead.")
    dataset = load_dataset("json", data_files="enhanced_dataset.json", split="train")

dataset = dataset.map(formatting_prompts_func, batched = True,)

print("üöÄ Initializing Trainer...")
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
    ),
)

trainer_stats = trainer.train()
print("üèÜ Training Complete on Combined Data!")



import re

FastLanguageModel.for_inference(model)

topic = "frequency response"
product = "DesignMax DM8SE"

question = f"What is the {topic} of the {product}?"
style_instruction = "Extract the specific value and answer in a single, plain English sentence. Do NOT use markdown tables, pipes (|), or lists."

prompt = alpaca_prompt.format(
    f"{question}\n\n{style_instruction}",
    "",
    "",
)

inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    use_cache=True,
    temperature=0.1,
)

def clean_output(text):
    response = text.split("### Response:")[-1].strip()

    response = response.replace("|", "")
    response = response.replace("---", "")

    response = re.sub(' +', ' ', response)

    response = re.sub(r'\'', '', response)

    return response.strip()

final_answer = clean_output(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])

print("\n" + "="*50)
print(f"ü§ñ AI ASSISTANT REPORT")
print("="*50)
print(f"üìù Query: {question}")
print("-" * 50)
print(f"üí¨ Answer:\n")
print(final_answer)
print("\n" + "="*50)





# !pip install llama-cpp-python



# import os
# import torch
# from unsloth import FastLanguageModel
# import subprocess
# import shutil

# print("üßπ Clearing GPU/CPU Memory...")
# import gc
# try:
#     del model
#     del tokenizer
# except:
#     pass
# gc.collect()
# torch.cuda.empty_cache()

# checkpoint_path = "outputs/checkpoint-60"
# merged_model_dir = "final_merged_model"
# gguf_filename = "Product_SLM_Llama3_v2.gguf"
# quantized_filename = "Product_SLM_Llama3_v2.Q4_K_M.gguf"

# print(f"üöÄ Starting High-RAM Export using checkpoint: {checkpoint_path}")

# try:
#     print("\nüîÑ Loading & Merging Model to Disk...")
#     model, tokenizer = FastLanguageModel.from_pretrained(
#         model_name = checkpoint_path,
#         max_seq_length = 2048,
#         dtype = None,
#         load_in_4bit = True,
#     )

#     model.save_pretrained_merged(
#         merged_model_dir,
#         tokenizer,
#         save_method = "merged_16bit",
#     )
#     print(f"   ‚úÖ Merged model saved to '{merged_model_dir}'")

#     del model
#     del tokenizer
#     gc.collect()
#     torch.cuda.empty_cache()

#     print("\nüõ†Ô∏è Setting up official llama.cpp tools...")
#     if not os.path.exists("llama.cpp"):
#         subprocess.run(["git", "clone", "https://github.com/ggerganov/llama.cpp"], check=True)
#         subprocess.run(["make", "-C", "llama.cpp", "clean"], check=True, capture_output=True)
#         subprocess.run(["make", "-C", "llama.cpp", "all"], check=True, capture_output=True)
#         subprocess.run(["pip", "install", "-r", "llama.cpp/requirements.txt"], check=True, capture_output=True)

#     print(f"\nüì¶ Converting {merged_model_dir} to GGUF (F16)...")
#     convert_cmd = [
#         "python", "llama.cpp/convert_hf_to_gguf.py",
#         merged_model_dir,
#         "--outfile", gguf_filename,
#         "--outtype", "f16"
#     ]
#     subprocess.run(convert_cmd, check=True)
#     print(f"   ‚úÖ Intermediate GGUF created: {gguf_filename}")

#     print(f"\nüìâ Quantizing to Q4_K_M...")
#     quantize_cmd = [
#         "./llama.cpp/llama-quantize",
#         gguf_filename,
#         quantized_filename,
#         "Q4_K_M"
#     ]
#     subprocess.run(quantize_cmd, check=True)

#     if os.path.exists(gguf_filename): os.remove(gguf_filename)
#     shutil.rmtree(merged_model_dir, ignore_errors=True)

#     print("\n" + "="*50)
#     print(f"üèÜ SUCCESS! High-RAM Export Complete.")
#     print("="*50)
#     print(f"File: {quantized_filename}")
#     print(f"Size: ~4.9 GB")
#     print("\n‚¨áÔ∏è  Starting Download (Check your browser)...")

#     from google.colab import files
#     files.download(quantized_filename)

# except Exception as e:
#     print(f"\n‚ùå Error: {e}")
#     print("Tip: If this fails, ensure you selected 'High RAM' in Runtime > Change Runtime Type")







import re
from unsloth import FastLanguageModel # Ensure this is imported

FastLanguageModel.for_inference(model)

topic = "frequency response"
product = "DesignMax DM8SE"

question = f"What is the {topic} of the {product}?"
style_instruction = "Extract the specific value and answer in a single, plain English sentence. Do NOT use markdown tables, pipes (|), or lists."

prompt = alpaca_prompt.format(
    f"{question}\n\n{style_instruction}",
    "",
    "",
)

inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

# 1. First Pass: Get the accurate data (even if messy)
outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    use_cache=True,
    temperature=0.1,
)

def clean_output(text):
    response = text.split("### Response:")[-1].strip()
    response = response.replace("|", "")
    response = response.replace("---", "")
    response = re.sub(' +', ' ', response)
    return response.strip()

raw_answer = clean_output(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])

# --- NEW ADDITION: THE CLEANER ---
# We use the base model to rewrite the messy data into a perfect sentence.
polishing_prompt = f"""Rewrite this technical data into a single, clean English sentence.
Data: {raw_answer}
Response:"""

with model.disable_adapter(): # Turn off fine-tuning temporarily to get better grammar
    inputs_polish = tokenizer([polishing_prompt], return_tensors="pt").to("cuda")
    outputs_polish = model.generate(**inputs_polish, max_new_tokens=128, use_cache=True, temperature=0.1)
    final_answer = tokenizer.batch_decode(outputs_polish, skip_special_tokens=True)[0].split("Response:")[-1].strip()
# ---------------------------------

print("\n" + "="*50)
print(f"ü§ñ AI ASSISTANT REPORT")
print("="*50)
print(f"üìù Query: {question}")
print("-" * 50)
print(f"üí¨ Answer:\n")
print(final_answer)
print("\n" + "="*50)

